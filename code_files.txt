

==================================================
File: D:\handwriting-recognition-system\ml/data/README.md
==================================================
# Dataset for Handwriting Recognition System

## Data Source
The dataset used for this handwriting recognition system is derived from the IAM Handwriting Database. This dataset contains forms of handwritten English text which can be used to train and test handwriting recognition systems.

## Preprocessing Steps
1. Image Normalization: All images are converted to grayscale and normalized to a standard size.
2. Noise Reduction: A Gaussian filter is applied to reduce noise in the images.
3. Binarization: Otsu's method is used for image binarization.
4. Data Augmentation: Random rotations, shifts, and zooms are applied to increase the dataset size and improve model generalization.

## Ethical Considerations
1. Privacy: All personal information has been removed from the dataset to protect individuals' privacy.
2. Bias: The dataset has been reviewed for potential biases in handwriting styles, but users should be aware that the model's performance may vary across different handwriting styles and languages.

## Limitations
1. Language: The current dataset is limited to English handwriting. Future versions aim to include multi-language support.
2. Handwriting Styles: While diverse, the dataset may not cover all possible handwriting styles, especially those of individuals with motor impairments.

## Usage
Researchers and developers using this dataset should cite the original IAM Handwriting Database in their work.

For more information on the IAM Handwriting Database, visit: https://fki.tic.heia-fr.ch/databases/iam-handwriting-database



==================================================
File: D:\handwriting-recognition-system\ml/inference/predict.py
==================================================
from ml.models.cnn_lstm_model import ctc_loss
import tensorflow as tf
import numpy as np
from ml.preprocessing.preprocess import preprocess_image
import yaml
import os
import logging

logger = logging.getLogger(__name__)

def load_model(model_path):
    return tf.keras.models.load_model(model_path, custom_objects={'ctc_loss': ctc_loss})

def predict(model, image, config):
    logger.info(f"Input image: shape={image.shape}, dtype={image.dtype}")
    
    preprocessed_image = preprocess_image(image, target_size=tuple(config['model']['input_shape'][:2]))
    if preprocessed_image is None:
        logger.error("Failed to preprocess the image")
        return "Error: Unable to preprocess the image."
    
    logger.info(f"Preprocessed image: shape={preprocessed_image.shape}, dtype={preprocessed_image.dtype}")
    
    # Ensure the image has the correct shape for the model
    if preprocessed_image.shape != tuple(config['model']['input_shape']):
        logger.warning(f"Resizing image to match model input shape: {tuple(config['model']['input_shape'])}")
        preprocessed_image = tf.image.resize(preprocessed_image, config['model']['input_shape'][:2])
        preprocessed_image = tf.expand_dims(preprocessed_image, axis=0)
    else:
        preprocessed_image = np.expand_dims(preprocessed_image, axis=0)
    
    logger.info(f"Model input: shape={preprocessed_image.shape}, dtype={preprocessed_image.dtype}")
    
    prediction = model.predict(preprocessed_image)
    logger.info(f"Model output: shape={prediction.shape}, dtype={prediction.dtype}")
    
    recognized_text = decode_prediction(prediction[0], config['model']['character_set'])
    
    return recognized_text

def decode_prediction(prediction, character_set):
    # Implement beam search decoding
    input_len = np.ones(prediction.shape[0]) * prediction.shape[1]
    results = tf.keras.backend.ctc_decode(prediction, input_length=input_len, greedy=False, beam_width=100, top_paths=1)
    
    # Extract the best path
    best_path = results[0][0]
    
    # Convert to text
    text = ''.join([character_set[index] for index in best_path[0] if index != -1 and index < len(character_set)])
    
    # Split text into lines
    lines = text.split('\n')
    
    return lines

# Load configuration
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
config_path = os.path.join(project_root, 'config', 'config.yaml')
with open(config_path, "r", encoding="utf-8") as config_file:
    config = yaml.safe_load(config_file)



==================================================
File: D:\handwriting-recognition-system\ml/models/cnn_lstm_model.py
==================================================
# This file is no longer needed for the Hugging Face implementation.
# However, we'll keep a placeholder here in case we need to add custom model logic in the future.

def create_advanced_cnn_lstm_model(input_shape, num_classes):
    # This function is no longer used with Hugging Face models
    pass

def ctc_loss(y_true, y_pred):
    # This function is no longer used with Hugging Face models
    pass



==================================================
File: D:\handwriting-recognition-system\ml/preprocessing/preprocess.py
==================================================
import cv2
import numpy as np
from PIL import Image
import io
from scipy import ndimage
import tensorflow as tf
import base64
import logging
import os

logger = logging.getLogger(__name__)

def preprocess_image(image_data, target_size=(128, 512)):
    try:
        # Handle different input types
        if isinstance(image_data, str):
            image_data = base64.b64decode(image_data)
        
        if isinstance(image_data, bytes):
            image = Image.open(io.BytesIO(image_data))
        elif isinstance(image_data, np.ndarray):
            image = Image.fromarray(image_data)
        elif isinstance(image_data, Image.Image):
            image = image_data
        else:
            raise ValueError("Unsupported image data type")

        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')

        # Resize
        image = image.resize(target_size, Image.LANCZOS)
        
        # Convert to numpy array
        image_array = np.array(image)
        
        # Convert to grayscale
        if image_array.ndim == 3:
            image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
        
        # Noise reduction using Non-Local Means denoising
        image_array = cv2.fastNlMeansDenoising(image_array, None, 10, 7, 21)
        
        # Contrast enhancement using CLAHE (Contrast Limited Adaptive Histogram Equalization)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        image_array = clahe.apply(image_array)
        
        # Binarization using Sauvola's method
        window_size = 25
        k = 0.2
        R = 128
        thresh_sauvola = cv2.threshold(image_array, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[0]
        mean = cv2.boxFilter(image_array, -1, (window_size, window_size))
        mean_square = cv2.boxFilter(image_array**2, -1, (window_size, window_size))
        variance = mean_square - mean**2
        threshold = mean * (1 + k * ((variance / R)**0.5 - 1))
        image_array = np.where(image_array > threshold, 255, 0).astype(np.uint8)
        
        # Skew correction using Hough Transform
        edges = cv2.Canny(image_array, 50, 150, apertureSize=3)
        lines = cv2.HoughLines(edges, 1, np.pi/180, 100)
        if lines is not None:
            angle = np.median([line[0][1] for line in lines])
            if angle > np.pi / 4:
                angle = angle - np.pi / 2
            (h, w) = image_array.shape[:2]
            center = (w // 2, h // 2)
            M = cv2.getRotationMatrix2D(center, angle * 180 / np.pi, 1.0)
            image_array = cv2.warpAffine(image_array, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
        
        # Normalize pixel values
        image_array = image_array.astype(np.float32) / 255.0
        
        # Add channel dimension
        image_array = np.expand_dims(image_array, axis=-1)
        
        return image_array
    
    except Exception as e:
        logger.error(f"Error in preprocessing: {str(e)}")
        return None

def segment_lines(image):
    # Implement line segmentation using projection profile
    horizontal_projection = np.sum(image, axis=1)
    line_boundaries = np.where(np.diff(horizontal_projection > 0))[0]
    lines = []
    for i in range(0, len(line_boundaries), 2):
        if i + 1 < len(line_boundaries):
            lines.append(image[line_boundaries[i]:line_boundaries[i+1], :])
    return lines

def segment_words(line_image):
    # Implement word segmentation using connected component analysis
    _, labels, stats, _ = cv2.connectedComponentsWithStats(line_image, connectivity=8)
    words = []
    for i in range(1, np.max(labels) + 1):
        x, y, w, h, area = stats[i]
        if area > 50:  # Minimum area threshold to avoid noise
            words.append(line_image[y:y+h, x:x+w])
    return words

def deskew(image):
    coords = np.column_stack(np.where(image > 0))
    angle = cv2.minAreaRect(coords)[-1]
    if angle < -45:
        angle = -(90 + angle)
    else:
        angle = -angle
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
    return rotated

def augment_image(image):
    # Random rotation
    angle = np.random.uniform(-15, 15)
    h, w = image.shape[:2]
    M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)
    image = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)
    
    # Random scaling
    scale = np.random.uniform(0.8, 1.2)
    image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)
    
    # Random translation
    tx = np.random.uniform(-5, 5)
    ty = np.random.uniform(-5, 5)
    M = np.float32([[1, 0, tx], [0, 1, ty]])
    image = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)
    
    # Random shear
    shear = np.random.uniform(-0.1, 0.1)
    M = np.float32([[1, shear, 0], [0, 1, 0]])
    image = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)
    
    # Elastic distortion
    image = elastic_transform(image, image.shape[1] * 2, image.shape[1] * 0.08, image.shape[1] * 0.08)
    
    return image

def elastic_transform(image, alpha, sigma, alpha_affine, random_state=None):
    if random_state is None:
        random_state = np.random.RandomState(None)

    shape = image.shape
    shape_size = shape[:2]
    
    # Random affine
    center_square = np.float32(shape_size) // 2
    square_size = min(shape_size) // 3
    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])
    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)
    M = cv2.getAffineTransform(pts1, pts2)
    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)

    dx = ndimage.gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha
    dy = ndimage.gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha
    
    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))
    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))

    return ndimage.map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)

def preprocess_dataset(dataset_path, target_size=(128, 512)):
    processed_images = []
    labels = []
    
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if file.endswith(('.png', '.jpg', '.jpeg')):
                image_path = os.path.join(root, file)
                label = os.path.basename(root)  # Assuming the folder name is the label
                
                image = Image.open(image_path)
                processed_image = preprocess_image(image, target_size)
                
                if processed_image is not None:
                    processed_images.append(processed_image)
                    labels.append(label)
    
    return np.array(processed_images), np.array(labels)

def generate_synthetic_data(num_samples, target_size=(128, 512)):
    synthetic_images = []
    synthetic_labels = []
    
    characters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*()_+-=[]{}|;:,.<>?`~∫∑∏≠≤≥∈∉"
    
    for _ in range(num_samples):
        image = np.random.rand(*target_size) * 255
        image = image.astype(np.uint8)
        label = ''.join(np.random.choice(list(characters), size=np.random.randint(5, 15)))
        
        processed_image = preprocess_image(image, target_size)
        if processed_image is not None:
            synthetic_images.append(processed_image)
            synthetic_labels.append(label)
    
    return np.array(synthetic_images), np.array(synthetic_labels)

if __name__ == "__main__":
    # Test the preprocessing function
    test_image = np.random.rand(100, 200, 3) * 255
    test_image = test_image.astype(np.uint8)
    
    preprocessed = preprocess_image(test_image)
    if preprocessed is not None:
        print(f"Preprocessed image shape: {preprocessed.shape}")
        print(f"Preprocessed image dtype: {preprocessed.dtype}")
        print(f"Preprocessed image min value: {preprocessed.min()}")
        print(f"Preprocessed image max value: {preprocessed.max()}")
    
    # Test with base64 encoded image
    import base64
    with open("path/to/test_image.jpg", "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode()
    
    preprocessed = preprocess_image(encoded_string)
    if preprocessed is not None:
        print(f"Preprocessed base64 image shape: {preprocessed.shape}")
    
    # Test the augmentation function
    augmented = augment_image(test_image)
    print(f"Augmented image shape: {augmented.shape}")

    # Test dataset preprocessing
    dataset_path = "path/to/IAM/dataset"
    processed_images, labels = preprocess_dataset(dataset_path)
    print(f"Processed {len(processed_images)} images with {len(labels)} labels")

    # Generate synthetic data
    synthetic_images, synthetic_labels = generate_synthetic_data(100)
    print(f"Generated {len(synthetic_images)} synthetic images with {len(synthetic_labels)} labels")



==================================================
File: D:\handwriting-recognition-system\ml/training/train.py
==================================================
import tensorflow as tf
import numpy as np
import os
from ml.models.cnn_lstm_model import create_advanced_cnn_lstm_model, ctc_loss
from ml.preprocessing.preprocess import preprocess_image, augment_image
import yaml
from sklearn.model_selection import train_test_split
import logging

logger = logging.getLogger(__name__)

def load_config():
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    config_path = os.path.join(project_root, 'config', 'config.yaml')
    with open(config_path, 'r') as config_file:
        return yaml.safe_load(config_file)

def load_dataset(data_dir, config):
    images = []
    labels = []
    character_set = config['model']['character_set']
    
    for char in character_set:
        char_dir = os.path.join(data_dir, char)
        if os.path.isdir(char_dir):
            for file in os.listdir(char_dir):
                if file.endswith(('.png', '.jpg', '.jpeg')):
                    img_path = os.path.join(char_dir, file)
                    img = preprocess_image(img_path, target_size=tuple(config['model']['input_shape'][:2]))
                    
                    if img is not None:
                        images.append(img)
                        labels.append(character_set.index(char))
    
    return np.array(images), np.array(labels)

def data_generator(x, y, batch_size, config, augment=True):
    num_samples = x.shape[0]
    num_classes = len(config['model']['character_set'])
    
    while True:
        indices = np.random.permutation(num_samples)
        for i in range(0, num_samples, batch_size):
            batch_indices = indices[i:i+batch_size]
            batch_x = x[batch_indices]
            batch_y = y[batch_indices]
            
            if augment:
                batch_x = np.array([augment_image(img) for img in batch_x])
            
            batch_y_onehot = tf.keras.utils.to_categorical(batch_y, num_classes)
            
            yield batch_x, batch_y_onehot

def train_model(config):
    # Load and preprocess data
    x, y = load_dataset(config['paths']['dataset'], config)
    
    # Split the data into training and validation sets
    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)
    
    # Create and compile the model
    model = create_advanced_cnn_lstm_model(tuple(config['model']['input_shape']), len(config['model']['character_set']))
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=config['training']['learning_rate']),
        loss=ctc_loss,
        metrics=['accuracy']
    )
    
    # Define callbacks
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(config['paths']['best_model'], save_best_only=True, monitor='val_accuracy'),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        tf.keras.callbacks.TensorBoard(log_dir=config['paths']['logs'])
    ]
    
    # Create data generators
    train_gen = data_generator(x_train, y_train, config['training']['batch_size'], config, augment=config['training']['data_augmentation'])
    val_gen = data_generator(x_val, y_val, config['training']['batch_size'], config, augment=False)
    
    # Train the model
    history = model.fit(
        train_gen,
        steps_per_epoch=len(x_train) // config['training']['batch_size'],
        validation_data=val_gen,
        validation_steps=len(x_val) // config['training']['batch_size'],
        epochs=config['training']['epochs'],
        callbacks=callbacks
    )
    
    # Save the final model
    model.save(config['paths']['model_save'])
    
    return history

if __name__ == "__main__":
    config = load_config()
    history = train_model(config)
    
    logger.info("Training completed. Model saved.")

    # Plot training history
    import matplotlib.pyplot as plt

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(config['paths']['logs'], 'training_history.png'))
    plt.close()

    logger.info("Training history plot saved.")


==================================================
File: D:\handwriting-recognition-system\backend/app/api/routes.py
==================================================
from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from app.services.ocr_service import recognize_text
from app.utils.logger import get_logger
import base64
import io
from PIL import Image
import traceback

router = APIRouter()
logger = get_logger(__name__)

@router.post("/recognize")
async def recognize_handwriting(file: UploadFile = File(...), model_type: str = 'ensemble'):
    try:
        contents = await file.read()
        if not contents:
            raise HTTPException(status_code=400, detail="Empty file")
        
        recognized_text, confidence = recognize_text(contents, model_type)
        logger.info(f"Successfully recognized text from uploaded image using {model_type} model")
        return {"text": recognized_text, "confidence": confidence}
    except Exception as e:
        logger.error(f"Error during text recognition: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/recognize_base64")
async def recognize_handwriting_base64(image_data: str = Form(...), model_type: str = 'ensemble'):
    try:
        if not image_data:
            raise HTTPException(status_code=400, detail="Empty image data")
        
        recognized_text, confidence = recognize_text(image_data, model_type)
        logger.info(f"Successfully recognized text from base64 image using {model_type} model")
        return {"text": recognized_text, "confidence": confidence}
    except Exception as e:
        logger.error(f"Error during text recognition: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/recognize_multiline")
async def recognize_multiline_handwriting(file: UploadFile = File(...), model_type: str = 'ensemble'):
    try:
        contents = await file.read()
        if not contents:
            raise HTTPException(status_code=400, detail="Empty file")
        
        recognized_text, confidence = recognize_text(contents, model_type)
        lines = recognized_text.split('\n')
        logger.info(f"Successfully recognized multi-line text from uploaded image using {model_type} model")
        return {"lines": lines, "confidence": confidence}
    except Exception as e:
        logger.error(f"Error during multi-line text recognition: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/recognize_realtime")
async def recognize_realtime(file: UploadFile = File(...), model_type: str = 'ensemble'):
    try:
        contents = await file.read()
        if not contents:
            raise HTTPException(status_code=400, detail="Empty file")
        
        recognized_text, confidence = recognize_text(contents, model_type)
        logger.info(f"Successfully recognized text in real-time using {model_type} model")
        return {"text": recognized_text, "confidence": confidence}
    except Exception as e:
        logger.error(f"Error during real-time text recognition: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

==================================================
File: D:\handwriting-recognition-system\backend/app/services/ocr_service.py
==================================================
import io
from PIL import Image
import numpy as np
import os
import sys
import yaml
import tensorflow as tf
import torch
from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BertTokenizer, BertForMaskedLM
import re
import base64
import traceback
from tenacity import retry, stop_after_attempt, wait_exponential

# Add the project root to PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, project_root)

from ml.preprocessing.preprocess import preprocess_image, segment_lines, segment_words
from app.utils.logger import get_logger

# Load configuration
config_path = os.path.join(project_root, "config", "config.yaml")
with open(config_path, "r", encoding="utf-8") as config_file:
    config = yaml.safe_load(config_file)

# Logger initialization
logger = get_logger(__name__)

# Global variables for models
processor = None
model_cache = {}
bert_tokenizer = None
bert_model = None

def get_model(model_type):
    if model_type == 'base':
        model_key = 'handwritten_base'
    elif model_type == 'small':
        model_key = 'handwritten_small'
    elif model_type == 'math':
        model_key = 'math'
    else:
        raise ValueError(f"Invalid model type: {model_type}")

    if model_key not in model_cache:
        model_name = config["huggingface"]["models"][model_key]
        model_cache[model_key] = load_model_with_retry(model_name)
    return model_cache[model_key]

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def load_model_with_retry(model_name):
    logger.info(f"Attempting to load model: {model_name}")
    try:
        model = VisionEncoderDecoderModel.from_pretrained(model_name)
        logger.info(f"Successfully loaded model: {model_name}")
        return model
    except Exception as e:
        logger.error(f"Error loading model {model_name}: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def initialize_models():
    global processor, bert_tokenizer, bert_model

    try:
        logger.info("Starting model initialization")
        processor = TrOCRProcessor.from_pretrained(config["huggingface"]["models"]["handwritten_base"])
        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')
        logger.info("Models loaded successfully")
    except Exception as e:
        logger.error(f"Error initializing models: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def recognize_trocr(image, model_type='base'):
    try:
        if image.mode != 'RGB':
            image = image.convert('RGB')

        pixel_values = processor(images=image, return_tensors="pt").pixel_values
        
        model = get_model(model_type)

        with torch.no_grad():
            generated_ids = model.generate(pixel_values)
        recognized_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

        return recognized_text
    except Exception as e:
        logger.error(f"Error in recognize_trocr: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def recognize_text(image_data, model_type='ensemble'):
    try:
        image = convert_to_pil_image(image_data)
        preprocessed_image = preprocess_image(image)
        
        if preprocessed_image is None:
            raise ValueError("Failed to preprocess the image")

        lines = segment_lines(preprocessed_image)
        recognized_lines = []

        for line in lines:
            words = segment_words(line)
            line_text = []
            for word in words:
                word_image = Image.fromarray((word * 255).astype(np.uint8))
                if model_type == 'ensemble':
                    results = []
                    for model in ['base', 'small', 'math']:
                        results.append(recognize_trocr(word_image, model))
                    word_text = ensemble_decision(results)
                else:
                    word_text = recognize_trocr(word_image, model_type)
                
                line_text.append(word_text)
            
            line_text = ' '.join(line_text)
            line_text = post_process_text(line_text)
            recognized_lines.append(line_text)

        return '\n'.join(recognized_lines)
    except Exception as e:
        logger.error(f"Error in recognize_text: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def convert_to_pil_image(image_data):
    if isinstance(image_data, bytes):
        return Image.open(io.BytesIO(image_data))
    elif isinstance(image_data, str):
        image_data = base64.b64decode(image_data)
        return Image.open(io.BytesIO(image_data))
    elif isinstance(image_data, np.ndarray):
        return Image.fromarray(image_data)
    elif isinstance(image_data, Image.Image):
        return image_data
    else:
        raise ValueError("Unsupported image data type")

def ensemble_decision(results):
    # Implement a more sophisticated ensemble decision
    # For now, we'll use the result with the highest confidence (longest text)
    return max(results, key=len)

def post_process_text(text):
    text = normalize_case(text)
    text = handle_special_characters(text)
    text = apply_grammar_rules(text)
    text = correct_spelling(text)
    return text

def normalize_case(text):
    # Convert to sentence case
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return ' '.join(sentence.capitalize() for sentence in sentences)

def handle_special_characters(text):
    special_chars = {
        '∫': '\\int',
        '∑': '\\sum',
        '∏': '\\prod',
        '≠': '\\neq',
        '≤': '\\leq',
        '≥': '\\geq',
        '∈': '\\in',
        '∉': '\\notin',
    }
    for char, replacement in special_chars.items():
        text = text.replace(char, replacement)
    return text

def apply_grammar_rules(text):
    # Implement basic grammar rules
    text = re.sub(r'\s+([.,!?])', r'\1', text)  # Remove spaces before punctuation
    text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)  # Add space between lowercase and uppercase letters
    return text

def correct_spelling(text):
    # Use BERT for spelling correction
    tokens = bert_tokenizer.tokenize(text)
    for i, token in enumerate(tokens):
        if token not in bert_tokenizer.vocab:
            masked_text = ' '.join(tokens[:i] + ['[MASK]'] + tokens[i+1:])
            input_ids = bert_tokenizer.encode(masked_text, return_tensors='pt')
            with torch.no_grad():
                outputs = bert_model(input_ids)
            predicted_token = bert_tokenizer.convert_ids_to_tokens(outputs.logits[0, i].argmax().item())
            tokens[i] = predicted_token
    return bert_tokenizer.convert_tokens_to_string(tokens)

# TensorFlow setup for GPU (if available)
def setup_gpu():
    try:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logger.info(f"GPU setup complete. Found {len(gpus)} GPU(s).")
        else:
            logger.info("No GPUs found. Using CPU.")
    except Exception as e:
        logger.error(f"Error setting up GPU: {str(e)}")
        logger.error(traceback.format_exc())

# Call GPU setup
setup_gpu()

initialize_models()

==================================================
File: D:\handwriting-recognition-system\backend/app/utils/logger.py
==================================================
import logging
import os
from logging.handlers import RotatingFileHandler
import yaml

def setup_logger(log_level, log_file):
    # Load configuration
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
    with open(os.path.join(project_root, "config", "config.yaml"), "r", encoding="utf-8") as config_file:
        config = yaml.safe_load(config_file)

    # Create logs directory if it doesn't exist
    log_dir = os.path.join(project_root, config['logging']['directory'])
    os.makedirs(log_dir, exist_ok=True)

    # Set up logging
    logger = logging.getLogger()
    logger.setLevel(log_level)

    # File handler
    file_handler = RotatingFileHandler(
        os.path.join(log_dir, log_file),
        maxBytes=config['logging']['max_size'],
        backupCount=config['logging']['backup_count']
    )
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # Console handler
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    return logger

def get_logger(name):
    return logging.getLogger(name)



==================================================
File: D:\handwriting-recognition-system\backend/main.py
==================================================
import sys
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import yaml
import traceback
import signal

# Add the project root to PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

# Load configuration
config_path = os.path.join(project_root, "config", "config.yaml")
with open(config_path, "r", encoding="utf-8") as config_file:
    config = yaml.safe_load(config_file)

from app.api.routes import router
from app.utils.logger import setup_logger
from app.services.ocr_service import initialize_models

# Setup logger
logger = setup_logger(config['logging']['level'], config['logging']['file'])

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include the router
app.include_router(router)

def signal_handler(signum, frame):
    logger.error(f"Received signal {signum}. Exiting...")
    sys.exit(1)

signal.signal(signal.SIGSEGV, signal_handler)

@app.on_event("startup")
async def startup_event():
    try:
        # Initialize models
        initialize_models()
        logger.info("Models initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing models: {str(e)}")
        logger.error(traceback.format_exc())
        logger.warning("Application will start, but some features may not work correctly.")

if __name__ == "__main__":
    try:
        logger.info(f"Starting server on {config['server']['host']}:{config['server']['port']}")
        uvicorn.run(app, host=config['server']['host'], port=int(config['server']['port']))
    except Exception as e:
        logger.error(f"Error starting server: {str(e)}")
        logger.error(traceback.format_exc())
        sys.exit(1)



==================================================
File: D:\handwriting-recognition-system\backend/requirements.txt
==================================================
absl-py==2.1.0
asgiref==3.8.1
astunparse==1.6.3
blinker==1.9.0
certifi==2024.8.30
charset-normalizer==3.4.0
click==8.1.7
colorama==0.4.6
contourpy==1.3.1
cycler==0.12.1
fastapi==0.68.0
filelock==3.16.1
Flask==3.1.0
flatbuffers==24.3.25
fonttools==4.55.2
fsspec==2024.10.0
gast==0.6.0
google-pasta==0.2.0
grpcio==1.68.1
h11==0.14.0
h5py==3.12.1
huggingface-hub==0.26.5
idna==3.10
iniconfig==2.0.0
itsdangerous==2.2.0
Jinja2==3.1.4
joblib==1.4.2
keras==3.7.0
kiwisolver==1.4.7
libclang==18.1.1
Markdown==3.7
markdown-it-py==3.0.0
MarkupSafe==3.0.2
matplotlib==3.9.3
mdurl==0.1.2
ml-dtypes==0.4.1
mpmath==1.3.0
namex==0.0.8
networkx==3.4.2
numpy==2.0.2
opencv-python==4.10.0.84
opt_einsum==3.4.0
optree==0.13.1
packaging==24.2
pillow==11.0.0
pluggy==1.5.0
protobuf==5.29.1
pydantic==1.10.19
Pygments==2.18.0
pyparsing==3.2.0
pytest==8.3.4
python-dateutil==2.9.0.post0
python-multipart==0.0.5
PyYAML==6.0.2
regex==2024.11.6
requests==2.32.3
rich==13.9.4
safetensors==0.4.5
scikit-learn==1.5.2
scipy==1.14.1
setuptools==75.6.0
six==1.17.0
starlette==0.14.2
sympy==1.13.1
tenacity==9.0.0
tensorboard==2.18.0
tensorboard-data-server==0.7.2
tensorflow==2.18.0
tensorflow_intel==2.18.0
termcolor==2.5.0
threadpoolctl==3.5.0
tokenizers==0.21.0
torch==2.5.1
tqdm==4.67.1
transformers==4.47.0
typing_extensions==4.12.2
urllib3==2.2.3
uvicorn==0.15.0
Werkzeug==3.1.3
wheel==0.45.1
wrapt==1.17.0


==================================================
File: D:\handwriting-recognition-system\config/config.yaml
==================================================
model:
  input_shape: [64, 256, 1]
  num_classes: 128
  character_set: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*()_+-=[]{}|;:,.<>?`~∫∑∏≠≤≥∈∉"

training:
  batch_size: 32
  epochs: 100
  learning_rate: 0.001
  data_augmentation: true

paths:
  dataset: "ml/data/handwriting_dataset"
  logs: "logs"
  best_model: "ml/models/best_model.h5"
  model_save: "ml/models/saved_model.h5"
  model_weights: "ml/models/model_weights.h5"

huggingface:
  api_key: "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"
  models:
    handwritten_base: "microsoft/trocr-base-handwritten"
    handwritten_small: "microsoft/trocr-small-handwritten"
    math: "fhswf/TrOCR_Math_handwritten"

server:
  host: "0.0.0.0"
  port: 8000

logging:
  level: "INFO"
  directory: "logs"
  file: "app.log"
  max_size: 5242880  # 5 MB
  backup_count: 3

environment: "development"

gpu:
  use_gpu: true
  memory_growth: true

preprocessing:
  target_size: [128, 512]
  noise_reduction:
    method: "fastNlMeansDenoising"
    h: 10
    template_window_size: 7
    search_window_size: 21
  contrast_adjustment:
    method: "clahe"
    clip_limit: 2.0
    tile_grid_size: [8, 8]
  binarization:
    method: "sauvola"
    window_size: 25
    k: 0.2
    R: 128
  skew_correction:
    method: "hough_transform"
    canny_threshold1: 50
    canny_threshold2: 150
    hough_threshold: 100

post_processing:
  spelling_correction:
    model: "bert-base-uncased"
  grammar_correction:
    enabled: true

==================================================
File: D:\handwriting-recognition-system\frontend/public/index.html
==================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <link rel="icon" href="/assets/icons/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
        name="description"
        content="Advanced Handwriting Recognition System for Specially Challenged Individuals"
    />
    <meta name="author" content="Your Name or Organization" />
    <meta name="keywords" content="handwriting recognition, accessibility, AI, machine learning" />
    <link rel="apple-touch-icon" href="/assets/icons/apple-icon.png" />
    <link rel="manifest" href="/manifest.json" />
    <title>Handwriting Recognition System</title>
</head>
<body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <script>
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', function() {
                navigator.serviceWorker.register('/service-worker.js')
                    .then(function(registration) {
                        console.log('ServiceWorker registration successful with scope: ', registration.scope);
                    })
                    .catch(function(err) {
                        console.log('ServiceWorker registration failed: ', err);
                    });
            });
        }
    </script>
</body>
</html>

==================================================
File: D:\handwriting-recognition-system\frontend/public/service-worker.js
==================================================
const CACHE_NAME = 'handwriting-recognition-v1';
const urlsToCache = [
  '/',
  '/index.html',
  '/static/js/bundle.js',
  '/static/css/main.css',
  '/manifest.json'
];

self.addEventListener('install', event => {
  event.waitUntil(
    caches.open(CACHE_NAME)
      .then(cache => {
        return cache.addAll(urlsToCache)
          .catch(error => {
            console.error('Failed to cache resources:', error);
          });
      })
  );
});

self.addEventListener('fetch', event => {
  event.respondWith(
    caches.match(event.request)
      .then(response => response || fetch(event.request))
  );
});

self.addEventListener('activate', event => {
  event.waitUntil(
    caches.keys().then(cacheNames => {
      return Promise.all(
        cacheNames.map(cacheName => {
          if (cacheName !== CACHE_NAME) {
            return caches.delete(cacheName);
          }
        })
      );
    })
  );
});

==================================================
File: D:\handwriting-recognition-system\frontend/src/components/ExportOptions.jsx
==================================================
import React from 'react';
import { Button } from "./ui/button"
import { Card, CardContent, CardHeader, CardTitle } from "./ui/card"
import { Copy, FileText, FileJson } from 'lucide-react'

const ExportOptions = ({ text }) => {
  const copyToClipboard = () => {
    navigator.clipboard.writeText(text).then(() => {
      alert('Text copied to clipboard!');
    });
  };

  const exportAsTxt = () => {
    const blob = new Blob([text], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'recognized_text.txt';
    a.click();
    URL.revokeObjectURL(url);
  };

  const exportAsJson = () => {
    const jsonData = JSON.stringify({ recognizedText: text });
    const blob = new Blob([jsonData], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'recognized_text.json';
    a.click();
    URL.revokeObjectURL(url);
  };

  return (
    <Card className="mt-8">
      <CardHeader>
        <CardTitle>Export Options</CardTitle>
      </CardHeader>
      <CardContent className="flex flex-wrap gap-4">
        <Button onClick={copyToClipboard}>
          <Copy className="mr-2 h-4 w-4" /> Copy to Clipboard
        </Button>
        <Button onClick={exportAsTxt}>
          <FileText className="mr-2 h-4 w-4" /> Export as TXT
        </Button>
        <Button onClick={exportAsJson}>
          <FileJson className="mr-2 h-4 w-4" /> Export as JSON
        </Button>
      </CardContent>
    </Card>
  );
};

export default ExportOptions;



==================================================
File: D:\handwriting-recognition-system\frontend/src/components/Footer.jsx
==================================================
import React from 'react';

const Footer = () => {
  return (
    <footer className="bg-gray-200 text-center py-4">
      <p>&copy; {new Date().getFullYear()} Advanced Handwriting Recognition System. All rights reserved.</p>
    </footer>
  );
};

export default Footer;



==================================================
File: D:\handwriting-recognition-system\frontend/src/components/Header.jsx
==================================================
import React from 'react';
import { Link } from 'react-router-dom';

const Header = () => {
  return (
    <header className="bg-blue-600 text-white">
      <div className="container mx-auto px-4 py-6">
        <nav className="flex justify-between items-center">
          <Link to="/" className="text-2xl font-bold">
            Handwriting Recognition
          </Link>
          <ul className="flex space-x-4">
            <li>
              <Link to="/" className="hover:underline">
                Home
              </Link>
            </li>
            <li>
              <Link to="/recognition" className="hover:underline">
                Recognition
              </Link>
            </li>
          </ul>
        </nav>
      </div>
    </header>
  );
};

export default Header;



==================================================
File: D:\handwriting-recognition-system\frontend/src/components/ImageUpload.jsx
==================================================
import React, { useRef, useState } from 'react';

const ImageUpload = ({ onUpload }) => {
  const [dragActive, setDragActive] = useState(false);
  const [preview, setPreview] = useState(null);
  const [error, setError] = useState(null);
  const inputRef = useRef(null);

  const handleDrag = (e) => {
    e.preventDefault();
    e.stopPropagation();
    if (e.type === 'dragenter' || e.type === 'dragover') {
      setDragActive(true);
    } else if (e.type === 'dragleave') {
      setDragActive(false);
    }
  };

  const handleDrop = (e) => {
    e.preventDefault();
    e.stopPropagation();
    setDragActive(false);
    if (e.dataTransfer.files && e.dataTransfer.files[0]) {
      handleFile(e.dataTransfer.files[0]);
    }
  };

  const handleChange = (e) => {
    e.preventDefault();
    if (e.target.files && e.target.files[0]) {
      handleFile(e.target.files[0]);
    }
  };

  const handleFile = (file) => {
    setError(null);
    if (file.type.startsWith('image/')) {
      setPreview(URL.createObjectURL(file));
      onUpload(file);
    } else {
      setError('Please upload a valid image file.');
    }
  };

  return (
    <div
      className={`border-2 border-dashed rounded-lg p-8 text-center ${
        dragActive ? 'border-blue-500 bg-blue-50' : 'border-gray-300'
      }`}
      onDragEnter={handleDrag}
      onDragLeave={handleDrag}
      onDragOver={handleDrag}
      onDrop={handleDrop}
    >
      <input
        ref={inputRef}
        type="file"
        accept="image/*"
        onChange={handleChange}
        className="hidden"
      />
      {preview ? (
        <div className="mb-4">
          <img src={preview} alt="Preview" className="max-w-full h-auto" />
        </div>
      ) : (
        <p className="mb-4">Drag and drop your image here, or click to select a file</p>
      )}
      <button
        onClick={() => inputRef.current.click()}
        className="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600 transition duration-300"
      >
        Select File
      </button>
      {error && <p className="text-red-500 mt-2">{error}</p>}
    </div>
  );
};

export default ImageUpload;



==================================================
File: D:\handwriting-recognition-system\frontend/src/pages/HomePage.jsx
==================================================
import React from 'react';
import { Link } from 'react-router-dom';

const HomePage = () => {
  return (
    <div className="text-center">
      <h1 className="text-4xl font-bold mb-6">Welcome to Handwriting Recognition</h1>
      <p className="mb-8">
        Our advanced system helps convert handwritten text into digital format with high accuracy.
      </p>
      <Link
        to="/recognition"
        className="bg-blue-600 text-white px-6 py-3 rounded-lg hover:bg-blue-700 transition duration-300"
      >
        Start Recognition
      </Link>
    </div>
  );
};

export default HomePage;



==================================================
File: D:\handwriting-recognition-system\frontend/src/pages/RecognitionPage.jsx
==================================================
import React, { useState, useCallback } from 'react';
import axios from 'axios';
import { Button } from "../components/ui/button"
import { Card, CardContent, CardHeader, CardTitle } from "../components/ui/card"
import { Tabs, TabsContent, TabsList, TabsTrigger } from "../components/ui/tabs"
import { Alert, AlertDescription, AlertTitle } from "../components/ui/alert"
import { Loader2 } from 'lucide-react'
import ImageUpload from '../components/ImageUpload';
import RecognizedText from '../components/RecognizedText';
import ExportOptions from '../components/ExportOptions';
import InteractiveCorrection from '../components/InteractiveCorrection';
import CameraCapture from '../components/CameraCapture';

const RecognitionPage = () => {
  const [recognizedText, setRecognizedText] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState(null);
  const [modelType, setModelType] = useState('ensemble');
  const [confidence, setConfidence] = useState(null);

  const handleRecognition = useCallback(async (file) => {
    setIsLoading(true);
    setError(null);
    try {
      const formData = new FormData();
      formData.append('file', file);
      formData.append('model_type', modelType);

      const response = await axios.post('http://localhost:8000/recognize', formData, {
        headers: {
          'Content-Type': 'multipart/form-data',
        },
        timeout: 60000, // 60 seconds timeout
      });

      setRecognizedText(response.data.text);
      setConfidence(response.data.confidence);
    } catch (error) {
      console.error('Error:', error);
      if (error.code === 'ECONNABORTED') {
        setError('Request timed out. Please try again.');
      } else if (error.response) {
        setError(`Server error: ${error.response.data.detail || 'Unknown error'}`);
      } else if (error.request) {
        setError('Unable to connect to the server. Please check if the backend is running and accessible.');
      } else {
        setError('An error occurred during recognition. Please try again.');
      }
      setRecognizedText('');
      setConfidence(null);
    } finally {
      setIsLoading(false);
    }
  }, [modelType]);

  const handleRealTimeCapture = useCallback(async (imageBlob) => {
    try {
      const formData = new FormData();
      formData.append('file', imageBlob);
      formData.append('model_type', modelType);

      const response = await axios.post('http://localhost:8000/recognize', formData, {
        headers: {
          'Content-Type': 'multipart/form-data',
        },
        timeout: 10000, // 10 seconds timeout for real-time recognition
      });

      setRecognizedText(response.data.text);
      setConfidence(response.data.confidence);
    } catch (error) {
      console.error('Real-time recognition error:', error);
      // Don't set error state for real-time recognition to avoid disrupting the UI
    }
  }, [modelType]);

  const handleTextCorrection = useCallback((correctedText) => {
    setRecognizedText(correctedText);
  }, []);

  return (
    <div className="container mx-auto px-4 py-8">
      <h1 className="text-3xl font-bold mb-6">Advanced Handwriting Recognition</h1>
      <Tabs defaultValue="upload" className="w-full">
        <TabsList className="grid w-full grid-cols-2">
          <TabsTrigger value="upload">Image Upload</TabsTrigger>
          <TabsTrigger value="camera">Camera Capture</TabsTrigger>
        </TabsList>
        <TabsContent value="upload">
          <Card>
            <CardHeader>
              <CardTitle>Upload Image</CardTitle>
            </CardHeader>
            <CardContent>
              <ImageUpload onUpload={handleRecognition} />
            </CardContent>
          </Card>
        </TabsContent>
        <TabsContent value="camera">
          <Card>
            <CardHeader>
              <CardTitle>Camera Capture</CardTitle>
            </CardHeader>
            <CardContent>
              <CameraCapture onCapture={handleRecognition} onRealTimeCapture={handleRealTimeCapture} />
            </CardContent>
          </Card>
        </TabsContent>
      </Tabs>
      <Card className="mt-8">
        <CardHeader>
          <CardTitle>Model Selection</CardTitle>
        </CardHeader>
        <CardContent>
          <select
            value={modelType}
            onChange={(e) => setModelType(e.target.value)}
            className="w-full p-2 border rounded"
          >
            <option value="ensemble">Ensemble (Default)</option>
            <option value="base">TrOCR Base</option>
            <option value="small">TrOCR Small</option>
            <option value="math">TrOCR Math</option>
          </select>
        </CardContent>
      </Card>
      {isLoading && (
        <div className="flex justify-center items-center mt-8">
          <Loader2 className="mr-2 h-4 w-4 animate-spin" />
          <span>Processing image...</span>
        </div>
      )}
      {error && (
        <Alert variant="destructive" className="mt-8">
          <AlertTitle>Error</AlertTitle>
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}
      {recognizedText && (
        <>
          <RecognizedText text={recognizedText} confidence={confidence} />
          <InteractiveCorrection text={recognizedText} onCorrection={handleTextCorrection} />
          <ExportOptions text={recognizedText} />
        </>
      )}
    </div>
  );
};

export default RecognitionPage;


==================================================
File: D:\handwriting-recognition-system\frontend/src/components/CameraCapture.jsx
==================================================


import React, { useRef, useState, useCallback, useEffect } from "react";
import axios from "axios";
import { Button } from "./ui/button";
import { Camera, RefreshCw, Play, Pause } from 'lucide-react';
import ExportOptions from './ExportOptions'; // Assuming ExportOptions is imported

const CameraCapture = ({ onCapture, onRealTimeCapture }) => {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const [isCameraOn, setIsCameraOn] = useState(false);
  const [isLoading, setIsLoading] = useState(false);
  const [capturedImage, setCapturedImage] = useState(null);
  const [recognizedText, setRecognizedText] = useState(null);
  const [isRealTimeMode, setIsRealTimeMode] = useState(false);

  const startCamera = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      videoRef.current.srcObject = stream;
      videoRef.current.play();
      setIsCameraOn(true);
    } catch (err) {
      console.error('Error accessing camera:', err);
      alert("Unable to access the camera. Please check permissions.");
    }
  }, []);

  const stopCamera = useCallback(() => {
    const stream = videoRef.current.srcObject;
    if (stream) {
      stream.getTracks().forEach((track) => track.stop());
    }
    videoRef.current.srcObject = null;
    setIsCameraOn(false);
    setIsRealTimeMode(false);
  }, []);

  const captureImage = async () => {
    setIsLoading(true);
    const canvas = canvasRef.current;
    const video = videoRef.current;

    if (canvas && video) {
      const ctx = canvas.getContext("2d");
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

      canvas.toBlob(async (blob) => {
        if (blob) {
          // Send captured image to backend for recognition
          try {
            const formData = new FormData();
            formData.append("file", blob);
            const response = await axios.post("http://localhost:8000/recognize_realtime", formData, {
              timeout: 10000 // 10 seconds timeout
            });
            setCapturedImage(URL.createObjectURL(blob));
            setRecognizedText(response.data.recognized_text); // Update recognized text state
            onCapture(response.data.recognized_text); // Pass recognized text to parent
          } catch (error) {
            console.error("Error during recognition:", error);
            alert("Failed to process the image. Please try again.");
          } finally {
            setIsLoading(false);
          }
        }
      }, "image/jpeg");
    }
    stopCamera();
  };

  const toggleRealTimeMode = useCallback(() => {
    setIsRealTimeMode((prev) => !prev);
  }, []);

  useEffect(() => {
    let interval;
    if (isRealTimeMode && isCameraOn) {
      interval = setInterval(() => {
        if (videoRef.current && canvasRef.current) {
          const context = canvasRef.current.getContext('2d');
          context.drawImage(videoRef.current, 0, 0, canvasRef.current.width, canvasRef.current.height);
          canvasRef.current.toBlob((blob) => {
            onRealTimeCapture(blob); // Send to parent component
            recognizeText(blob); // Real-time recognition
          }, 'image/jpeg');
        }
      }, 1000); // Capture every second
    }
    return () => {
      if (interval) clearInterval(interval);
    };
  }, [isRealTimeMode, isCameraOn, onRealTimeCapture]);

  const recognizeText = async (imageBlob) => {
    setIsLoading(true);
    const formData = new FormData();
    formData.append('file', imageBlob);

    try {
      const response = await axios.post('http://localhost:8000/recognize', formData, {
        headers: { 'Content-Type': 'multipart/form-data' },
      });
      setRecognizedText(response.data.recognized_text); // Save recognized text
    } catch (err) {
      console.error('Error during text recognition:', err);
    } finally {
      setIsLoading(false);
    }
  };

  const retakePhoto = useCallback(() => {
    setCapturedImage(null);
    setRecognizedText(null);
    startCamera();
  }, [startCamera]);

  return (
    <div className="camera-capture">
      <div className="status-badge">
        {isCameraOn ? (
          <span className="badge badge-green">Camera On</span>
        ) : (
          <span className="badge badge-red">Camera Off</span>
        )}
      </div>
      <div className="video-container">
        <video ref={videoRef} className="video-feed"></video>
        <canvas ref={canvasRef} className="hidden"></canvas>
      </div>
      <div className="controls">
        {!isCameraOn ? (
          <Button onClick={startCamera} className="w-full">
            <Camera className="mr-2 h-4 w-4" /> Start Camera
          </Button>
        ) : (
          <Button onClick={stopCamera} className="btn btn-danger">
            Stop Camera
          </Button>
        )}
        <Button
          onClick={captureImage}
          className="btn btn-success"
          disabled={!isCameraOn || isLoading}
        >
          {isLoading ? "Processing..." : "Capture Image"}
        </Button>
        <Button onClick={toggleRealTimeMode} className="btn btn-secondary">
          {isRealTimeMode ? (
            <Pause className="mr-2 h-4 w-4" />
          ) : (
            <Play className="mr-2 h-4 w-4" />
          )}
          {isRealTimeMode ? "Stop Real-time" : "Start Real-time"}
        </Button>
      </div>
      {capturedImage && (
        <div className="captured-image-container">
          <img src={capturedImage} alt="Captured" className="w-full h-auto" />
          <Button
            onClick={retakePhoto}
            className="absolute bottom-4 left-1/2 transform -translate-x-1/2"
          >
            <RefreshCw className="mr-2 h-4 w-4" /> Retake Photo
          </Button>
        </div>
      )}
      {recognizedText && !isLoading && (
        <div className="mt-4">
          <h3>Recognized Text:</h3>
          <p>{recognizedText}</p>
          <ExportOptions recognizedText={recognizedText} /> {/* Export options for DOCX, PDF */}
        </div>
      )}
    </div>
  );
};

export default CameraCapture;




==================================================
File: D:\handwriting-recognition-system\frontend/src/App.jsx
==================================================
import React from 'react';
import { createBrowserRouter, RouterProvider } from 'react-router-dom';
import Header from './components/Header';
import Footer from './components/Footer';
import HomePage from './pages/HomePage';
import RecognitionPage from './pages/RecognitionPage';

const router = createBrowserRouter([
  {
    path: "/",
    element: (
      <>
        <Header />
        <HomePage />
        <Footer />
      </>
    ),
  },
  {
    path: "/recognition",
    element: (
      <>
        <Header />
        <RecognitionPage />
        <Footer />
      </>
    ),
  },
], {
  future: {
    v7_startTransition: true,
    v7_relativeSplatPath: true,
    v7_fetcherPersist: true,
    v7_normalizeFormMethod: true,
    v7_partialHydration: true,
    v7_skipActionErrorRevalidation: true,
  },
});

const App = () => {
  return (
    <RouterProvider router={router} />
  );
};

export default App;



==================================================
File: D:\handwriting-recognition-system\frontend/src/index.js
==================================================
import React from 'react';
import { createRoot } from 'react-dom/client';
import './index.css';
import App from './App';

const container = document.getElementById('root');
const root = createRoot(container);

root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);