==================================================
Folder: D:/handwriting-recognition-system/backend
==================================================
Directory structure

main.py
README.md
requirements.txt
app\api\routes.py
app\api\__pycache__\routes.cpython-312.pyc
app\models\__init__.py
app\services\ocr_service.py
app\services\tests\test_ocr_service.py
app\services\tests\__pycache__\test_api.cpython-312-pytest-8.3.4.pyc
app\services\__pycache__\ocr_service.cpython-312.pyc
app\utils\logger.py
app\utils\__pycache__\logger.cpython-312.pyc


==================================================
File: D:/handwriting-recognition-system/backend\main.py
==================================================
import sys
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import yaml
import traceback
import signal

# Add the project root to PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

# Load configuration
config_path = os.path.join(project_root, "config", "config.yaml")
with open(config_path, "r", encoding="utf-8") as config_file:
    config = yaml.safe_load(config_file)

from app.api.routes import router
from app.utils.logger import setup_logger
from app.services.ocr_service import initialize_models

# Setup logger
logger = setup_logger(config['logging']['level'], config['logging']['file'])

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include the router
app.include_router(router)

def signal_handler(signum, frame):
    logger.error(f"Received signal {signum}. Exiting...")
    sys.exit(1)

signal.signal(signal.SIGSEGV, signal_handler)

@app.on_event("startup")
async def startup_event():
    try:
        # Initialize models
        initialize_models()
        logger.info("Models initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing models: {str(e)}")
        logger.error(traceback.format_exc())
        logger.warning("Application will start, but some features may not work correctly.")

if __name__ == "__main__":
    try:
        logger.info(f"Starting server on {config['server']['host']}:{config['server']['port']}")
        uvicorn.run(app, host=config['server']['host'], port=int(config['server']['port']))
    except Exception as e:
        logger.error(f"Error starting server: {str(e)}")
        logger.error(traceback.format_exc())
        sys.exit(1)



==================================================
File: D:/handwriting-recognition-system/backend\README.md
==================================================
# Handwriting Recognition Backend

This is the backend for the Advanced Handwriting Recognition System. It provides an API for processing handwritten text images and returning the recognized text.

## Features

- FastAPI-based REST API
- Integration with machine learning model for text recognition
- Image preprocessing

## Getting Started

1. Install dependencies:
pip install -r requirements.txt

2. Start the server:
uvicorn main:app --reload

3. The API will be available at `http://localhost:8000`

## API Endpoints

- `POST /recognize`: Upload an image file to recognize handwritten text

## Testing

Run the tests using:
pytest


## Learn More

To learn FastAPI, check out the [FastAPI documentation](https://fastapi.tiangolo.com/).

==================================================
File: D:/handwriting-recognition-system/backend\requirements.txt
==================================================
absl-py==2.1.0
asgiref==3.8.1
astunparse==1.6.3
blinker==1.9.0
certifi==2024.8.30
charset-normalizer==3.4.0
click==8.1.7
colorama==0.4.6
contourpy==1.3.1
cycler==0.12.1
fastapi==0.68.0
filelock==3.16.1
Flask==3.1.0
flatbuffers==24.3.25
fonttools==4.55.2
fsspec==2024.10.0
gast==0.6.0
google-pasta==0.2.0
grpcio==1.68.1
h11==0.14.0
h5py==3.12.1
huggingface-hub==0.26.5
idna==3.10
iniconfig==2.0.0
itsdangerous==2.2.0
Jinja2==3.1.4
joblib==1.4.2
keras==3.7.0
kiwisolver==1.4.7
libclang==18.1.1
Markdown==3.7
markdown-it-py==3.0.0
MarkupSafe==3.0.2
matplotlib==3.9.3
mdurl==0.1.2
ml-dtypes==0.4.1
mpmath==1.3.0
namex==0.0.8
networkx==3.4.2
numpy==2.0.2
opencv-python==4.10.0.84
opt_einsum==3.4.0
optree==0.13.1
packaging==24.2
pillow==11.0.0
pluggy==1.5.0
protobuf==5.29.1
pydantic==1.10.19
Pygments==2.18.0
pyparsing==3.2.0
pytest==8.3.4
python-dateutil==2.9.0.post0
python-multipart==0.0.5
PyYAML==6.0.2
regex==2024.11.6
requests==2.32.3
rich==13.9.4
safetensors==0.4.5
scikit-learn==1.5.2
scipy==1.14.1
setuptools==75.6.0
six==1.17.0
starlette==0.14.2
sympy==1.13.1
tenacity==9.0.0
tensorboard==2.18.0
tensorboard-data-server==0.7.2
tensorflow==2.18.0
tensorflow_intel==2.18.0
termcolor==2.5.0
threadpoolctl==3.5.0
tokenizers==0.21.0
torch==2.5.1
tqdm==4.67.1
transformers==4.47.0
typing_extensions==4.12.2
urllib3==2.2.3
uvicorn==0.15.0
Werkzeug==3.1.3
wheel==0.45.1
wrapt==1.17.0


==================================================
File: D:/handwriting-recognition-system/backend\app/api/routes.py
==================================================
from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from app.services.ocr_service import recognize_text
from app.utils.logger import get_logger
import base64
import io
from PIL import Image
import traceback

router = APIRouter()
logger = get_logger(__name__)

@router.post("/recognize")
async def recognize_handwriting(file: UploadFile = File(...), model_type: str = 'ensemble'):
    try:
        contents = await file.read()
        if not contents:
            raise HTTPException(status_code=400, detail="Empty file")
        
        recognized_text, confidence = recognize_text(contents, model_type)
        logger.info(f"Successfully recognized text from uploaded image using {model_type} model")
        return {"text": recognized_text, "confidence": confidence}
    except Exception as e:
        logger.error(f"Error during text recognition: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/recognize_base64")
async def recognize_handwriting_base64(image_data: str = Form(...), model_type: str = 'ensemble'):
    try:
        if not image_data:
            raise HTTPException(status_code=400, detail="Empty image data")
        
        recognized_text, confidence = recognize_text(image_data, model_type)
        logger.info(f"Successfully recognized text from base64 image using {model_type} model")
        return {"text": recognized_text, "confidence": confidence}
    except Exception as e:
        logger.error(f"Error during text recognition: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/recognize_multiline")
async def recognize_multiline_handwriting(file: UploadFile = File(...), model_type: str = 'ensemble'):
    try:
        contents = await file.read()
        if not contents:
            raise HTTPException(status_code=400, detail="Empty file")
        
        recognized_text, confidence = recognize_text(contents, model_type)
        lines = recognized_text.split('\n')
        logger.info(f"Successfully recognized multi-line text from uploaded image using {model_type} model")
        return {"lines": lines, "confidence": confidence}
    except Exception as e:
        logger.error(f"Error during multi-line text recognition: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/recognize_realtime")
async def recognize_realtime(file: UploadFile = File(...), model_type: str = 'ensemble'):
    try:
        contents = await file.read()
        if not contents:
            raise HTTPException(status_code=400, detail="Empty file")
        
        recognized_text, confidence = recognize_text(contents, model_type)
        logger.info(f"Successfully recognized text in real-time using {model_type} model")
        return {"text": recognized_text, "confidence": confidence}
    except Exception as e:
        logger.error(f"Error during real-time text recognition: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

==================================================
File: D:/handwriting-recognition-system/backend\app/models/__init__.py
==================================================


==================================================
File: D:/handwriting-recognition-system/backend\app/services/ocr_service.py
==================================================
import io
from PIL import Image
import numpy as np
import os
import sys
import yaml
import tensorflow as tf
import torch
from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BertTokenizer, BertForMaskedLM
import re
import base64
import traceback
from tenacity import retry, stop_after_attempt, wait_exponential

# Add the project root to PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, project_root)

from ml.preprocessing.preprocess import preprocess_image, segment_lines, segment_words
from app.utils.logger import get_logger

# Load configuration
config_path = os.path.join(project_root, "config", "config.yaml")
with open(config_path, "r", encoding="utf-8") as config_file:
    config = yaml.safe_load(config_file)

# Logger initialization
logger = get_logger(__name__)

# Global variables for models
processor = None
model_cache = {}
bert_tokenizer = None
bert_model = None

def get_model(model_type):
    if model_type == 'base':
        model_key = 'handwritten_base'
    elif model_type == 'small':
        model_key = 'handwritten_small'
    elif model_type == 'math':
        model_key = 'math'
    else:
        raise ValueError(f"Invalid model type: {model_type}")

    if model_key not in model_cache:
        model_name = config["huggingface"]["models"][model_key]
        model_cache[model_key] = load_model_with_retry(model_name)
    return model_cache[model_key]

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def load_model_with_retry(model_name):
    logger.info(f"Attempting to load model: {model_name}")
    try:
        model = VisionEncoderDecoderModel.from_pretrained(model_name)
        logger.info(f"Successfully loaded model: {model_name}")
        return model
    except Exception as e:
        logger.error(f"Error loading model {model_name}: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def initialize_models():
    global processor, bert_tokenizer, bert_model

    try:
        logger.info("Starting model initialization")
        processor = TrOCRProcessor.from_pretrained(config["huggingface"]["models"]["handwritten_base"])
        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')
        logger.info("Models loaded successfully")
    except Exception as e:
        logger.error(f"Error initializing models: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def recognize_trocr(image, model_type='base'):
    try:
        if image.mode != 'RGB':
            image = image.convert('RGB')

        pixel_values = processor(images=image, return_tensors="pt").pixel_values
        
        model = get_model(model_type)

        with torch.no_grad():
            generated_ids = model.generate(pixel_values)
        recognized_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

        return recognized_text
    except Exception as e:
        logger.error(f"Error in recognize_trocr: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def recognize_text(image_data, model_type='ensemble'):
    try:
        image = convert_to_pil_image(image_data)
        preprocessed_image = preprocess_image(image)
        
        if preprocessed_image is None:
            raise ValueError("Failed to preprocess the image")

        lines = segment_lines(preprocessed_image)
        recognized_lines = []

        for line in lines:
            words = segment_words(line)
            line_text = []
            for word in words:
                word_image = Image.fromarray((word * 255).astype(np.uint8))
                if model_type == 'ensemble':
                    results = []
                    for model in ['base', 'small', 'math']:
                        results.append(recognize_trocr(word_image, model))
                    word_text = ensemble_decision(results)
                else:
                    word_text = recognize_trocr(word_image, model_type)
                
                line_text.append(word_text)
            
            line_text = ' '.join(line_text)
            line_text = post_process_text(line_text)
            recognized_lines.append(line_text)

        return '\n'.join(recognized_lines)
    except Exception as e:
        logger.error(f"Error in recognize_text: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def convert_to_pil_image(image_data):
    if isinstance(image_data, bytes):
        return Image.open(io.BytesIO(image_data))
    elif isinstance(image_data, str):
        image_data = base64.b64decode(image_data)
        return Image.open(io.BytesIO(image_data))
    elif isinstance(image_data, np.ndarray):
        return Image.fromarray(image_data)
    elif isinstance(image_data, Image.Image):
        return image_data
    else:
        raise ValueError("Unsupported image data type")

def ensemble_decision(results):
    # Implement a more sophisticated ensemble decision
    # For now, we'll use the result with the highest confidence (longest text)
    return max(results, key=len)

def post_process_text(text):
    text = normalize_case(text)
    text = handle_special_characters(text)
    text = apply_grammar_rules(text)
    text = correct_spelling(text)
    return text

def normalize_case(text):
    # Convert to sentence case
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return ' '.join(sentence.capitalize() for sentence in sentences)

def handle_special_characters(text):
    special_chars = {
        '∫': '\\int',
        '∑': '\\sum',
        '∏': '\\prod',
        '≠': '\\neq',
        '≤': '\\leq',
        '≥': '\\geq',
        '∈': '\\in',
        '∉': '\\notin',
    }
    for char, replacement in special_chars.items():
        text = text.replace(char, replacement)
    return text

def apply_grammar_rules(text):
    # Implement basic grammar rules
    text = re.sub(r'\s+([.,!?])', r'\1', text)  # Remove spaces before punctuation
    text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)  # Add space between lowercase and uppercase letters
    return text

def correct_spelling(text):
    # Use BERT for spelling correction
    tokens = bert_tokenizer.tokenize(text)
    for i, token in enumerate(tokens):
        if token not in bert_tokenizer.vocab:
            masked_text = ' '.join(tokens[:i] + ['[MASK]'] + tokens[i+1:])
            input_ids = bert_tokenizer.encode(masked_text, return_tensors='pt')
            with torch.no_grad():
                outputs = bert_model(input_ids)
            predicted_token = bert_tokenizer.convert_ids_to_tokens(outputs.logits[0, i].argmax().item())
            tokens[i] = predicted_token
    return bert_tokenizer.convert_tokens_to_string(tokens)

# TensorFlow setup for GPU (if available)
def setup_gpu():
    try:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logger.info(f"GPU setup complete. Found {len(gpus)} GPU(s).")
        else:
            logger.info("No GPUs found. Using CPU.")
    except Exception as e:
        logger.error(f"Error setting up GPU: {str(e)}")
        logger.error(traceback.format_exc())

# Call GPU setup
setup_gpu()

initialize_models()

==================================================
File: D:/handwriting-recognition-system/backend\app/services/tests/test_ocr_service.py
==================================================
import pytest
from app.services.ocr_service import recognize_text, handle_special_characters
from PIL import Image
import io
import numpy as np

def create_test_image(text):
    image = Image.new('RGB', (100, 30), color='white')
    return image

@pytest.mark.parametrize("model_type", ['cnn_lstm', 'trocr_base', 'trocr_small', 'trocr_math'])
def test_recognize_text(model_type):
    test_image = create_test_image("Hello")
    img_byte_arr = io.BytesIO()
    test_image.save(img_byte_arr, format='PNG')
    img_byte_arr = img_byte_arr.getvalue()
    
    result = recognize_text(img_byte_arr, model_type)
    assert isinstance(result, str)
    assert len(result) > 0

def test_handle_special_characters():
    input_text = "∫x dx + ∑i=1^n ai ≠ 0"
    expected_output = "\\int x dx + \\sum i=1^n ai \\neq 0"
    assert handle_special_characters(input_text) == expected_output

def test_recognize_text_invalid_model():
    test_image = create_test_image("Hello")
    img_byte_arr = io.BytesIO()
    test_image.save(img_byte_arr, format='PNG')
    img_byte_arr = img_byte_arr.getvalue()
    
    with pytest.raises(ValueError):
        recognize_text(img_byte_arr, 'invalid_model')



==================================================
File: D:/handwriting-recognition-system/backend\app/utils/logger.py
==================================================
import logging
import os
from logging.handlers import RotatingFileHandler
import yaml

def setup_logger(log_level, log_file):
    # Load configuration
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
    with open(os.path.join(project_root, "config", "config.yaml"), "r", encoding="utf-8") as config_file:
        config = yaml.safe_load(config_file)

    # Create logs directory if it doesn't exist
    log_dir = os.path.join(project_root, config['logging']['directory'])
    os.makedirs(log_dir, exist_ok=True)

    # Set up logging
    logger = logging.getLogger()
    logger.setLevel(log_level)

    # File handler
    file_handler = RotatingFileHandler(
        os.path.join(log_dir, log_file),
        maxBytes=config['logging']['max_size'],
        backupCount=config['logging']['backup_count']
    )
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # Console handler
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    return logger

def get_logger(name):
    return logging.getLogger(name)

